---


#--------------------------------------------------------------------------------
#phase 2 -  steps to do on *ONLY* on control pane
# that too only on 1 control plane and not the rest, if we have any
# on the rest of control panes, we will run the join as master later on
#
#configuring cluster using kubeadm
#
# see the 6443 refused below and maybe do the (below section called -6443-) containerd config change and restart here itself
#
#kubeadm init    (and do with --v=5)
# this has given me diff types of errors all the time during pre-flight checks, like such - 
#     sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --apiserver-advertise-address=172.31.100.23 --v=5
#     sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --kubernetes-version 1.26.0 --v=5   (this one i did for 1.26 version worked on 07-May-2023, best to use this version for now)

#           [preflight] Running pre-flight checks                                                                           
#           error execution phase preflight: [preflight] Some fatal errors occurred:                                        
#                   [ERROR CRI]: container runtime is not running: output: time="2023-05-03T09:21:13Z" level=fatal msg="validate service connection: CRI v1 runtime API is not implemented for endpoint \"unix:///var/run/containerd/containe
#           rd.sock\": rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService"                    
#           , error: exit status 1
# i followed 1 article which told to mv /etc/containerd/config.toml to config.toml.bak and restart containerd and then do kubeadm and that seems to have worked for me  (but that was for 1.27 kubectl/let/adm install, i did not have do this for 1.26 version)

# -6443-
# The connection to the server 192.168.1.2:6443 was refused - did you specify the right host or port?
# above was one of the worst errors that gave me a lot of trouble, i think i spent nearly 20 full hours tblshootting and re-installing again and again
# below links helped me with fixing it
# https://discuss.kubernetes.io/t/the-connection-to-the-server-192-168-1-2-6443-was-refused-did-you-specify-the-right-host-or-port/22260
# https://www.linuxtechi.com/install-kubernetes-cluster-on-debian/
# https://stackoverflow.com/questions/70849989/kube-apiserver-docker-shutting-down-got-signal-terminated/74695838#74695838
# https://www.vnoob.com/2022/12/kubectl-6443-connection-refused/
# i backed up and went to /etc/containerd/config.toml around line 125 (only 1 reference for SystemdCgroup) and changed SystemdCgroup = true and bounced containerd
# i see that it was also clearly mentioned in k8 docs itself on https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd-systemd


#do the kube config 3 steps
#   mkdir -p $HOME/.kube
#   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
#   sudo chown $(id -u):$(id -g) $HOME/.kube/config
#
#save the kubeadm join command info into a text file for LATER (will need to run it on the worker nodes) and make sure to RUN IT AS SUDO
#
#--------------------------------------------------------------------------------
#--------------------------------------------------------------------------------

#--------------------------------------------------------------------------------
#
### none of above are working. just place holders for now
#
# verify cluster on *ONLY* on master 
#
#run some commands like
#kubectl get nodes  (should show NotReady for now, as CNI not yet setup)
#
#--------------------------------------------------------------------------------
# install calico networking compoment (CNI) on *ONLY* on master
#
#lets use calico and do the kubectl apply of calico file
# https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises
# $ kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/tigera-operator.yaml
# $ kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.25.1/manifests/custom-resources.yaml
# $ watch kubectl get pods -n calico-system
# Wait until each pod has the STATUS of Running. we will see like below - 
NAME                                       READY   STATUS    RESTARTS      AGE
calico-kube-controllers-6bb86c78b4-dqvb5   1/1     Running   0             26m
calico-node-xdbjw                          1/1     Running   1 (12m ago)   26m
calico-typha-ff45cf5cb-g46wv               1/1     Running   2 (11m ago)   26m
csi-node-driver-h77d4                      2/2     Running   0             26m

$ kubectl get pods -A
NAMESPACE          NAME                                       READY   STATUS    RESTARTS       AGE
calico-apiserver   calico-apiserver-7d895bc6bd-5qtxj          1/1     Running   0              11m
calico-apiserver   calico-apiserver-7d895bc6bd-rtjz6          1/1     Running   0              11m
calico-system      calico-kube-controllers-6bb86c78b4-dqvb5   1/1     Running   0              27m
calico-system      calico-node-xdbjw                          1/1     Running   1 (13m ago)    27m
calico-system      calico-typha-ff45cf5cb-g46wv               1/1     Running   2 (12m ago)    27m
calico-system      csi-node-driver-h77d4                      2/2     Running   0              27m
kube-system        coredns-787d4945fb-kmbhm                   1/1     Running   0              30m
kube-system        coredns-787d4945fb-s4l9b                   1/1     Running   0              30m
kube-system        etcd-acg-k8-controlp1                      1/1     Running   24 (13m ago)   31m
kube-system        kube-apiserver-acg-k8-controlp1            1/1     Running   24 (13m ago)   31m
kube-system        kube-controller-manager-acg-k8-controlp1   1/1     Running   26 (13m ago)   31m
kube-system        kube-proxy-kwzrq                           1/1     Running   1 (13m ago)    30m
kube-system        kube-scheduler-acg-k8-controlp1            1/1     Running   25 (13m ago)   31m
tigera-operator    tigera-operator-5d6845b496-dtnvd           1/1     Running   2 (12m ago)    27m

# not sure below is needed ? 
4. Remove the taints on the control plane so that you can schedule pods on it.
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
kubectl taint nodes --all node-role.kubernetes.io/master-

$ ip a | egrep "group|inet"
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    inet 127.0.0.1/8 scope host lo
    inet6 ::1/128 scope host
2: ens5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc mq state UP group default qlen 1000
    inet 172.31.104.228/20 metric 100 brd 172.31.111.255 scope global dynamic ens5
    inet6 2a05:d01c:2c7:d801:ab88:a159:21da:4037/128 scope global dynamic noprefixroute
    inet6 fe80::8f8:bcff:feb2:a6ae/64 scope link
3: cali493b3880927@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP group default
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
4: caliefc135607e3@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP group default
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
5: cali1391198ee61@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP group default
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
6: cali8ff4067ca74@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP group default
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
7: cali3a69c20bc8f@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP group default
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
8: cali18cabf282fc@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UP group default
    inet6 fe80::ecee:eeff:feee:eeee/64 scope link
11: vxlan.calico: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8951 qdisc noqueue state UNKNOWN group default
    inet 192.168.189.0/32 scope global vxlan.calico
    inet6 fe80::6493:67ff:fea7:2b4f/64 scope link

#
#then check by doing kubectl get pods -o wide
#
#now all the nodes should show ready
#
#we will see a new network interface (maybe called tun10@NONE) which will be on the 192.168.X.X subnet if we had configured our kubadm init with that network
#
#--------------------------------------------------------------------------------
## https://www.digitalocean.com/community/tutorials/how-to-create-a-kubernetes-cluster-using-kubeadm-on-ubuntu-20-04
#--------------------------------------------------------------------------------
#--------------------------------------------------------------------------------
#--------------------------------------------------------------------------------




https://stackoverflow.com/questions/43379415/how-can-i-list-the-taints-on-kubernetes-nodes


kubectl get nodes -o json | jq '.items[].spec'

which will give the complete spec with node name, or:

kubectl get nodes -o json | jq '.items[].spec.taints'

will produce the list of the taints per each node

cloud_user@acg-k8-controlp1:~
$ kubectl get nodes -o json | jq '.items[].spec'
{
  "podCIDR": "192.168.0.0/24",
  "podCIDRs": [
    "192.168.0.0/24"
  ],
  "taints": [
    {
      "effect": "NoSchedule",
      "key": "node-role.kubernetes.io/control-plane"
    }
  ]
}
cloud_user@acg-k8-controlp1:~
$
cloud_user@acg-k8-controlp1:~
$ kubectl get nodes -o json | jq '.items[].spec.taints'
[
  {
    "effect": "NoSchedule",
    "key": "node-role.kubernetes.io/control-plane"
  }
]
cloud_user@acg-k8-controlp1:~
$

cloud_user@acg-k8-controlp1:~
$ kubectl taint nodes --all node-role.kubernetes.io/control-plane-
kubectl taint nodes --all node-role.kubernetes.io/master-
node/acg-k8-controlp1 untainted
error: taint "node-role.kubernetes.io/master" not found
cloud_user@acg-k8-controlp1:~
$

cloud_user@acg-k8-controlp1:~
$ kubectl get nodes -o wide
NAME               STATUS   ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION    CONTAINER-RUNTIME
acg-k8-controlp1   Ready    control-plane   45m   v1.26.0   172.31.104.228   <none>        Ubuntu 22.04.2 LTS   5.19.0-1024-aws   containerd://1.6.12
cloud_user@acg-k8-controlp1:~
$












